{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guipalazzo87/ISLP_labs/blob/main/Guilherme.Exerc%C3%ADcio_Pr%C3%A1tico_Context_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "497db21d-4e08-42a7-9258-90fecced929f",
      "metadata": {
        "id": "497db21d-4e08-42a7-9258-90fecced929f"
      },
      "source": [
        "# Exercício 1 - Token Embeddings (Operações e contextualização)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TgejuBG0gFF-",
      "metadata": {
        "id": "TgejuBG0gFF-"
      },
      "source": [
        "Conforme visto em sala de aula, existem várias técnicas de tokenização: word token, subword token, character token, byte token, dentre outras que podem obter resultados mistos. Esse exercício abordará embeddings para word tokens, ou word embeddings. Nesse contexto, siga as instruções:\n",
        "\n",
        "**a)** escolha no mínimo 2 grupos de palavras relacionadas, cada grupo contendo no mínimo 3 palavras.\n",
        "Ex.:\n",
        "\n",
        "*   Palavras relacionadas a paisagem\n",
        "    *   ceu\n",
        "    *   montanha\n",
        "    *   rio\n",
        "*   Palavras relacionadas a estudo\n",
        "    *   caderno\n",
        "    *   escola\n",
        "    *   livro\n",
        "\n",
        "**b)** Em seguida, obtenha os embeddings das palavras.\n",
        "\n",
        "**c)** Utilize algum método de redução de dimensionalidade para reduzir os vetores dos embeddings para apenas 2 dimensões.\n",
        "\n",
        "**d)** Faça o \"plot\" em um plano bidimensional com os valores obtidos, comparando os resultados referentes aos diferentes modelos.\n",
        "\n",
        "**e)** Execute operações de soma e subtração com os vetores e observe os resultados obtidos.\n",
        "\n",
        "**f)** Para comparar outras formas de gerar embeddings, utilize o modelo BERT de forma a gerar word embeddings a partir de frases e veja o resultado obtido."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc50c2b6-0745-4952-83c8-170275da0e2d",
      "metadata": {
        "id": "bc50c2b6-0745-4952-83c8-170275da0e2d"
      },
      "source": [
        "## Sugestão: Utilize bibliotecas para ter acesso a modelos pré-treinados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d993bfb-80c1-4817-8622-a065d71da414",
      "metadata": {
        "id": "2d993bfb-80c1-4817-8622-a065d71da414"
      },
      "source": [
        "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ⏳ <b>O código a seguir instala bibliotecas para permitir utilizar os embeddings do GloVe, Word2Vec e BERT</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uj9XEaqrmtbJ",
      "metadata": {
        "id": "uj9XEaqrmtbJ"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zCCjPAZgfVnm",
      "metadata": {
        "id": "zCCjPAZgfVnm"
      },
      "source": [
        "A seguir, exemplo de código utilizando a lib gensim para carregar vetores de modelos pré-treinados como GloVe e Word2vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b7afc1-6894-42e0-a3c9-479372f1da38",
      "metadata": {
        "id": "d2b7afc1-6894-42e0-a3c9-479372f1da38"
      },
      "outputs": [],
      "source": [
        "# Suprimindo warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sugestão de utilização\n",
        "# Importando libs necessárias\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Imprimindo a lista de modelos disponíveis em gensim-data\n",
        "print(list(api.info()['models'].keys()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WxZD_1Eof54i",
      "metadata": {
        "id": "WxZD_1Eof54i"
      },
      "source": [
        "Os modelos sugeridos GloVe e Word2vec estão disponíveis a partir de 'glove-wiki-gigaword-100' e 'word2vec-google-news-300', respectivamente. Deve demorar alguns minutos dependendo do tamanho do modelo que você escolher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u8QW27aUf4aC",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8QW27aUf4aC",
        "outputId": "21a340ab-9fcf-49d0-8807-c1d316924e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# Carregue os vetores dos modelos pré-treinados utilizando o método \"load()\" do objeto api.\n",
        "#TODO\n",
        "glove = api.load(...)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n2ivn_0dDM8j",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2ivn_0dDM8j",
        "outputId": "86158913-47b8-48e6-c35f-afecf3aca93d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(300,)\n"
          ]
        }
      ],
      "source": [
        "# Verificar o tamanho do vetor (dimensões) de uma palavra\n",
        "print(glove['king'].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ABpl1JD_Bi9v",
      "metadata": {
        "id": "ABpl1JD_Bi9v"
      },
      "source": [
        "# **EXERCÍCIO**\n",
        "# **a)** Escolha suas palavras, inicialmente em idioma inglês."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b4d461-4905-4a3b-9f49-5373cfa64b7e",
      "metadata": {
        "id": "80b4d461-4905-4a3b-9f49-5373cfa64b7e"
      },
      "outputs": [],
      "source": [
        "# Palavras em inglês\n",
        "words = [\"king\", \"princess\", \"monarch\", \"throne\", \"crown\",\n",
        "         \"mountain\", \"ocean\", \"tv\", \"rainbow\", \"cloud\", \"queen\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ewjGktYyCNqe",
      "metadata": {
        "id": "ewjGktYyCNqe"
      },
      "source": [
        "Agora obtenha os embeddings do modelo carregado anteriormente acessando a chave no vetor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qY-c731LHVLG",
      "metadata": {
        "id": "qY-c731LHVLG"
      },
      "outputs": [],
      "source": [
        "# O modelo funciona como um dicionário, acesse a chave correspondente ao token\n",
        "\n",
        "#TODO defina abaixo a variavel emb_palavras, contendo a lista de embeddings\n",
        "# de 'words'. Se quiser saber o embedding de 'casa', por exemplo, faz-se glove['casa']\n",
        "emb_palavras =\n",
        "\n",
        "# Imprimindo os 5 primeiros valores para cada embedding\n",
        "print(emb_palavras[:, :5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aDbbD4at4eNw",
      "metadata": {
        "collapsed": true,
        "id": "aDbbD4at4eNw"
      },
      "outputs": [],
      "source": [
        "print(words)\n",
        "print(emb_palavras)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hw8OM0oaDYC6",
      "metadata": {
        "id": "hw8OM0oaDYC6"
      },
      "source": [
        "Teste os modelos carregados tentando imprimir algumas palavras em português e veja o que acontece. Lembre-se que o modelo foi treinado sem caracteres acentuados ou case do caractere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VXWLCQLjDm8k",
      "metadata": {
        "id": "VXWLCQLjDm8k"
      },
      "outputs": [],
      "source": [
        "print(glove['terra'].shape)\n",
        "\n",
        "# print(w2v_v['gente'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CbPGzSLg6PWZ",
      "metadata": {
        "id": "CbPGzSLg6PWZ"
      },
      "source": [
        "Perceba que estamos utilizando modelos pré-treinados com corpus do idioma inglês. Portanto, no corpus ficaram faltando muitas palavras de outros idiomas como o português.\n",
        "\n",
        "Faça o download do arquivo pré-treinado em Word2Vec do FastText no endereço https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.pt.vec e carregue-o em uma variável.\n",
        "\n",
        "**Sugestão:** Faça o download com o comando wget e utilize o método load_word2vec_format() do KeyedVectors da lib gensim.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ohwUJg0XzB",
      "metadata": {
        "id": "c3ohwUJg0XzB"
      },
      "outputs": [],
      "source": [
        "# Faça o download do arquivo\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.pt.vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84K8Vt1dGOW0",
      "metadata": {
        "id": "84K8Vt1dGOW0"
      },
      "outputs": [],
      "source": [
        "# Carregue o modelo a partir do arquivo baixado\n",
        "caminho = 'wiki.pt.vec'\n",
        "modelo = KeyedVectors.load_word2vec_format(caminho, binary=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XUrvljrqLyBh",
      "metadata": {
        "id": "XUrvljrqLyBh"
      },
      "source": [
        "## Escolha suas palavras agora em português e salve em um vetor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V4UfcNYJGHMZ",
      "metadata": {
        "id": "V4UfcNYJGHMZ"
      },
      "outputs": [],
      "source": [
        "palavras = [\"rei\", \"princesa\", \"monarca\", \"trono\", \"coroa\",\n",
        "         \"montanha\", \"oceano\", \"tv\", \"chuva\", \"nuvem\", \"rainha\", \"caderno\", \"escola\", \"recreio\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0msepkrPDnIX",
      "metadata": {
        "id": "0msepkrPDnIX"
      },
      "source": [
        "# **EXERCÍCIO**\n",
        "# **b)** Crie um novo vetor que receberá os embeddings correspondentes às palavras em português que você escolheu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdy9ZAUYCkI3",
      "metadata": {
        "id": "fdy9ZAUYCkI3"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "embeddings_pt ="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kUyHMDDCNT0U",
      "metadata": {
        "id": "kUyHMDDCNT0U"
      },
      "source": [
        "# **EXERCÍCIO**\n",
        "# **c)** Utilize algum método de redução de dimensionalidade para reduzir os vetores dos embeddings para apenas 2 dimensões.\n",
        "\n",
        "**Sugestão:** Utilize o PCA do sklearn passando como parâmetro n_components=2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf89b540-550a-47cb-a4a5-502d6544d949",
      "metadata": {
        "id": "bf89b540-550a-47cb-a4a5-502d6544d949"
      },
      "outputs": [],
      "source": [
        "# Importando libs necessárias\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Exemplo de redução para 10 dimensões\n",
        "pca = PCA(n_components=10)\n",
        "vectors_pca = pca.fit_transform(embeddings_pt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Agora reduza para 2 dimensões usando o método PCA\n",
        "#TODO\n",
        "pca =\n",
        "vectors_pca ="
      ],
      "metadata": {
        "id": "gG-VUOJzHdDz"
      },
      "id": "gG-VUOJzHdDz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7TZ_90YZTI0G",
      "metadata": {
        "id": "7TZ_90YZTI0G"
      },
      "source": [
        "# **EXERCÍCIO**\n",
        "# **d)** Faça o \"plot\" em um plano bidimensional com os valores obtidos, se possível, comparando os resultados referentes a diferentes modelos.\n",
        "\n",
        "**Sugestão:** Utilize o código da célula abaixo como exemplo de como efetuar o plot, sendo necessário ajustar a variável que contém os vetores PCA, bem como os rótulos com as palavras selecionadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "359e1719-dd9b-4130-93f1-8cd2cf97e2d6",
      "metadata": {
        "id": "359e1719-dd9b-4130-93f1-8cd2cf97e2d6"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
        "axes.scatter(vectors_pca[:, 0], vectors_pca[:, 1])\n",
        "for i, p in enumerate(palavras):\n",
        "    axes.annotate(p, (vectors_pca[i, 0]+.02, vectors_pca[i, 1]+.02))\n",
        "axes.set_title('PCA Word Embeddings')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lrNHReUr99R7",
      "metadata": {
        "id": "lrNHReUr99R7"
      },
      "source": [
        "# Agora que você já sabe, faça um teste com outras palavras e veja como fica o plot do gráfico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HRxguhEy9-L5",
      "metadata": {
        "id": "HRxguhEy9-L5"
      },
      "outputs": [],
      "source": [
        "# Inicie aqui definindo suas variáveis\n",
        "#TODO\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9pWy_5FVAWE9",
      "metadata": {
        "id": "9pWy_5FVAWE9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8f6541fd-4b2c-40f3-b52b-feb052f8154b",
      "metadata": {
        "id": "8f6541fd-4b2c-40f3-b52b-feb052f8154b"
      },
      "source": [
        "# Word2Vec algebra\n",
        "# **EXERCÍCIO**\n",
        "# **e)** Execute operações de soma e subtração com os vetores e observe os resultados obtidos.\n",
        "\n",
        "**Sugestão:** Você pode utilizar o método \"most_similar()\" do modelo, utilizando os parâmetros \"positive\" e \"negative\" para operações de soma e subtração. O parâmetro \"topn\" controla a quantidade de resultados.\n",
        "\n",
        "**Observação:** Caso prefira utilizar numpy para as operações, ao chamar o método \"most_similar()\" utilize o parâmetro \"vector\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2fdb948-7ee0-46c6-8423-1bb6c1160bf7",
      "metadata": {
        "id": "c2fdb948-7ee0-46c6-8423-1bb6c1160bf7"
      },
      "outputs": [],
      "source": [
        "# Defina as palavras\n",
        "palavra1 = 'rei'\n",
        "palavra2 = 'homem'\n",
        "palavra3 = 'mulher'\n",
        "\n",
        "# Efetue a operação algébrica usando most_similar\n",
        "# O resultado esperado é uma palavra similar a 'rainha'\n",
        "resultado = modelo.most_similar(positive=[palavra1, palavra3], negative=[palavra2], topn=1)\n",
        "\n",
        "\n",
        "# Exiba o resultado da busca por similaridade\n",
        "print(f\"A palavra mais próxima da operação {palavra1} - {palavra2} + {palavra3} é: '{resultado[0][0]}'\")\n",
        "print(f\"com similaridade {resultado[0][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TmIJjnrABP40",
      "metadata": {
        "id": "TmIJjnrABP40"
      },
      "source": [
        "## Espaço para testar mais operações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fGbaDAm-BI2N",
      "metadata": {
        "id": "fGbaDAm-BI2N"
      },
      "outputs": [],
      "source": [
        "# Agora faça você\n",
        "\n",
        "# Defina as palavras\n",
        "#TODO\n",
        "\n",
        "# Efetue a operação algébrica\n",
        "#TODO\n",
        "\n",
        "# Exiba o resultado da busca por similaridade\n",
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca418d70-69ae-479c-b174-f61a69a834ba",
      "metadata": {
        "id": "ca418d70-69ae-479c-b174-f61a69a834ba"
      },
      "source": [
        "# Word2Vec vs BERT: Embeddings em contextos diferentes\n",
        "\n",
        "Nos modelos representativos mais complexos, como o BERT, o cáculo dos valores de embedding de uma palavra pode depender dramáticamente da aplicação na frase. Isso não é por acaso e faz total sentido, pois a semântica pode ser totalmente diferente.\n",
        "\n",
        "Por exemplo, observe a plavra 'manga' nas duas senteças abaixo:\n",
        "\n",
        "\n",
        "*   \"Sujei a manga da minha camisa.\"\n",
        "*   \"Quero comer manga com leite.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o5WYol4iBTY2",
      "metadata": {
        "id": "o5WYol4iBTY2"
      },
      "source": [
        "## Vamos testar o quanto o modelo consegue capturar o contexto na tokenização incluindo agora o BERT.\n",
        "\n",
        "# **EXERCÍCIO**\n",
        "# **f)** Para comparar outras formas de gerar embeddings, utilize o modelo BERT de forma a gerar word embeddings a partir de frases e veja o resultado obtido.\n",
        "\n",
        "O código a seguir vai auxiliar na instanciação do modelo BERT e definição de uma função para ter acesso facilitado aos embeddings do modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e02a644c-9413-407a-aa6b-d5a5079dbb76",
      "metadata": {
        "id": "e02a644c-9413-407a-aa6b-d5a5079dbb76"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Função para obter os embeddings do BERT\n",
        "def get_bert_embeddings(sentence, word):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    word_tokens = tokenizer.tokenize(sentence)\n",
        "    word_index = word_tokens.index(word)\n",
        "    word_embedding = last_hidden_states[0, word_index + 1, :]\n",
        "    return word_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uynd92ZhCf0p",
      "metadata": {
        "id": "uynd92ZhCf0p"
      },
      "source": [
        "# **f)** (Detalhamento) Defina duas sentenças que contenham uma mesma palavra em ambas, porém com semântica diferente. Em seguide calcule os embeddings da mesma palavra nas duas sentenças, para finalmente comparar os resultados.\n",
        "\n",
        "**Dica:** Utilize o método definido anteriormente, passando a sentença e a palavra que você deseja obter o embedding."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defina suas sentenças\n",
        "sentenca1 = \"the bank has a lot of money\"\n",
        "sentenca2 = \"i am sitting on a bank\"\n",
        "\n",
        "# Defina a palavra foco\n",
        "palavra_foco = 'bank'\n",
        "\n",
        "\n",
        "# Calcule os embeddings da palavra nas duas situações\n",
        "es1 = get_bert_embeddings(sentenca1, palavra_foco)\n",
        "es2 = get_bert_embeddings(sentenca2, palavra_foco)\n",
        "\n",
        "# Imprima os embeddings calculados\n",
        "print(\"Embedding da sentença 1:\", es1)\n",
        "print(\"Embedding da sentença 2:\", es2)\n",
        "\n",
        "\n",
        "# Calcule a similaridade\n",
        "# calcular a similaridade\n",
        "similaridade = cosine_similarity(es1.reshape(1, -1).detach().numpy(), es2.reshape(1, -1).detach().numpy())\n",
        "print(\"Similaridade:\", similaridade)"
      ],
      "metadata": {
        "id": "sl7fKcN2MQeq"
      },
      "id": "sl7fKcN2MQeq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agora é com você:"
      ],
      "metadata": {
        "id": "mPxwGKKaMVc0"
      },
      "id": "mPxwGKKaMVc0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ASaCTNgabCGh",
      "metadata": {
        "id": "ASaCTNgabCGh"
      },
      "outputs": [],
      "source": [
        "# Defina suas sentenças\n",
        "#TODO\n",
        "sentenca1 = \"\"\n",
        "sentenca2 = \"\"\n",
        "\n",
        "# Defina a palavra foco\n",
        "#TODO\n",
        "palavra_foco =\n",
        "\n",
        "\n",
        "# Calcule os embeddings da palavra nas duas situações\n",
        "#TODO. Use o método get_bert_embeddings definido acima\n",
        "\n",
        "# Imprima os embeddings calculados\n",
        "\n",
        "\n",
        "# Calcule a similaridade\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explicação da linha de similaridade\n",
        "\n",
        "O trecho de código `cosine_similarity(es1.reshape(1, -1).detach().numpy(), es2.reshape(1, -1).detach().numpy())` calcula a **similaridade de cosseno** entre os embeddings `es1` e `es2`.\n",
        "\n",
        "\n",
        "*   **`es1` e `es2`**: São os embeddings da palavra foco nas duas sentenças, obtidos do modelo BERT. Eles são tensores do PyTorch.\n",
        "*   **`.detach()`**: Este método desanexa o tensor do gráfico de computação atual, o que significa que não calcularemos gradientes para este tensor. Isso é útil quando você não precisa de backpropagation, como neste caso, onde estamos apenas comparando embeddings.\n",
        "*   **`.numpy()`**: Converte o tensor PyTorch para um array NumPy. A função `cosine_similarity` do sklearn espera arrays NumPy como entrada.\n",
        "*   **`.reshape(1, -1)`**: Redimensiona o array NumPy. O `reshape(1, -1)` transforma o array em um array 2D com 1 linha e tantas colunas quanto forem necessárias (-1). Isso é necessário porque a função `cosine_similarity` do sklearn espera uma entrada 2D (mesmo que você esteja comparando apenas um par de vetores).\n",
        "*   **`cosine_similarity(...)`**: Esta função do módulo `sklearn.metrics.pairwise` calcula a similaridade de cosseno entre dois vetores ou matrizes de vetores. A **similaridade de cosseno** mede o ângulo entre dois vetores. Um valor de 1 indica que os vetores são idênticos em direção (mesmo que a magnitude seja diferente), 0 indica que são ortogonais (não relacionados), e -1 indica que são completamente opostos.\n",
        "\n",
        "Em resumo, o código está obtendo os embeddings BERT para a palavra foco em duas sentenças diferentes, convertendo-os para um formato compatível com a biblioteca scikit-learn e, em seguida, calculando a similaridade de cosseno para ver quão semelhantes são esses embeddings. Um valor de similaridade de cosseno próximo a 1 indica que os embeddings são muito parecidos, sugerindo que a palavra tem um significado semelhante em ambos os contextos. Um valor próximo a 0 ou negativo sugere que a palavra tem significados diferentes."
      ],
      "metadata": {
        "id": "Df_scnycMzNf"
      },
      "id": "Df_scnycMzNf"
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}